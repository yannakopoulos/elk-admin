input {
    file {
        path => "/tmpscratch/users/matze/ttH/v29/work_queue.log"
        start_position => beginning
        ignore_older => 0

    	# this makes logstash read files even if it already has a record of
    	# reading them - good for testing, but delete when monitoring for real
        sincedb_path => "/dev/null"
    }
}

filter {
    csv {
        columns => [
            "timestamp",
    	    "total_workers_connected",
    	    "workers_init",
    	    "workers_idle",
    	    "workers_busy",
    	    "total_workers_joined",
    	    "total_workers_removed",
    	    "tasks_waiting",
    	    "tasks_running",
    	    "tasks_complete",
    	    "total_tasks_dispatched",
    	    "total_tasks_complete",
    	    "total_tasks_cancelled",
    	    "start_time",
    	    "total_send_time",
    	    "total_receive_time",
    	    "total_bytes_sent",
    	    "total_bytes_received",
    	    "efficiency",
    	    "idle_percentage",
    	    "capacity",
    	    "bandwidth",
    	    "total_cores",
    	    "total_memory",
    	    "total_disk",
    	    "total_gpus",
    	    "min_cores",
    	    "max_cores",
    	    "min_memory",
    	    "max_memory",
    	    "min_disk",
    	    "max_disk",
    	    "min_gpus",
    	    "max_gpus",
    	    "total_execute_time",
    	    "total_good_execute_time"
	    ]

        separator => " "
    }

    # get rid of header row
    if ([timestamp] == "#timestamp") {
        drop { }
    }

    mutate {
        # remove raw message field and empty column at end
	remove_field => [ "message", "column37" ]

        # round down times from microseconds to milliseconds for elasticsearch
        gsub => [ "timestamp", "\d{3}$", "" ]
	gsub => [ "start_time", "\d{3}$", "" ]

	# convert fields to appropriate types
	convert => {
	    "total_workers_connected" => "integer"
	    "workers_init" => "integer"
	    "workers_idle" => "integer"
	    "workers_busy" => "integer"
	    "total_workers_joined" => "integer"
	    "total_workers_removed" => "integer"
	    "tasks_waiting" => "integer"
	    "tasks_running" => "integer"
	    "tasks_complete" => "integer"
	    "total_tasks_dispatched" => "integer"
	    "total_tasks_complete" => "integer"
	    "total_tasks_cancelled" => "integer"
	    "total_send_time" => "integer"
	    "total_receive_time" => "integer"
	    "total_bytes_sent" => "integer"
	    "total_bytes_received" => "integer"
	    "efficiency" => "float"
	    "idle_percentage" => "float"
	    "capacity" => "float"
	    "bandwidth" => "float"
	    "total_cores" => "integer"
	    "total_memory" => "integer"
	    "total_disk" => "integer"
	    "total_gpus" => "integer"
	    "min_cores" => "integer"
	    "max_cores" => "integer"
	    "min_memory" => "integer"
	    "max_memory" => "integer"
	    "min_disk" => "integer"
	    "max_disk" => "integer"
	    "min_gpus" => "integer"
	    "max_gpus" => "integer"
	    "total_execute_time" => "integer"
	    "total_good_execute_time" => "integer"
	}
    }

    # parse additional timestamps
    date {
        match => [ "start_time", "UNIX_MS" ]
        target => "start_time"
    }

    date {
        match => [ "timestamp", "UNIX_MS" ]
	    target => "timestamp"
    }
}

output {
    elasticsearch {
        index => "[template]_lobster_work_queue"
    }

    # remove to not be swamped with confirmation messages in terminal
    stdout {
    }
}
